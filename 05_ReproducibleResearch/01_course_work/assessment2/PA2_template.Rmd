---
title: "Coursera Reproducible Research - Assessment 2"
author: "TechnoPhobe01"
date: "March 8, 2015"
output:
  html_document:
    fig_caption: yes
    number_sections: yes
    toc: yes
  pdf_document:
    fig_caption: yes
---

```{r setOptions}
require(knitr)
opts_chunk$set(fig.path="figure/")
opts_chunk$set(echo = TRUE, fig.width=10, fig.height=8)
```

# Report Analysis of [NOAA][1] Storm Database and Weather Events
*Author:* Technophobe  
*Date:* "March 8, 2015"

## Synopsis
In this report we aim to describe the impact of severe weather events in the United States between the years 1950 to 2011. This analysis is intended to address the following questions:

- Across the United States, which types of events (as indicated in the eventType variable) are most harmful with respect to population health?
- Across the United States, which types of events have the greatest economic consequences?

Our overall hypothesis is that severe weather events have increased in severity. To investigate and answer these questions and validate our hypothesis, we obtained the Storm and Weather events database from the National Oceanic and Atmospheric Administration ([NOAA][1])  which is collected from various sources across the U.S. We specifically obtained data for the years 1950 and 2011. 

Note: Additional documentation is avaialble from NOOA that explains how the data has been obtained, and what and how the variables are defined and constructed.

- [NOAA Event Database Website][2]
- [National Weather Service Storm Data Documentation][3]
- [National Climatic Data Center Storm Events FAQ][4]

Please refer to the **Storm Data Overview** and **Results** sections for a review of how the data is structureed, subsequently cleaned and interpreted.

# Result Summary

...

# Data Processing

## Setup environment 

The first step is to load the required R packages

```{r Setup, cache=FALSE}
setwd("~/dev/cousera/courses/05_ReproducibleResearch/01_course_work/assessment2")

requiredPackages <- c("R.utils",   # Used for unziping the bz2 zip file... 
                      "lubridate", # Used for time formatting  
                      "ggplot2",   # Used to plot graphics 
                      "dplyr",     # Used for data manipulation (very Cool!)
                      "scales",    # Used to scale map data to aesthetics,  
                                   # and provide methods for automatically  
                                   # determining breaks and labels for axes 
                                   # and legends.
                      "reshape2",  # Used to manipulate and reshape the data
                      "gridExtra", # Used to map out plots in Grids
                      "ggthemes",  # Extra themes, scales and geoms for ggplot (Very cool!)
                      "xtable")    # Used to print R objects HTML tables)
ipak <- function(pkg){
        new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
        if (length(new.pkg))
                install.packages(new.pkg, dependencies = TRUE)
        sapply(pkg, require, character.only = TRUE)
}

ipak(requiredPackages)
```
### Download and load the storm data

We download and extract the target zip file into the **./data** sub-directory, that we create if it does not exist. We then load the data into a data frame in preparation for manipulation. 

**Note:** This program assumes that all the data will be downloaded into the 'data' directory below the working directory.

The data for this assignment is downloaded from the course [web site][5]:

- **Dataset**: [Storm Data [**47Mb**]][6]

```{r loadData, cache=TRUE}
dataDir         <- "./data"
fileUrl         <- "https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2"
fileName        <- "storm.csv"                  # Extracted File Name
filePath        <- file.path(dataDir, fileName) # Filepath to extracted file

# If the data dir does not exist we create it, so it can be used to store the
# downloaded data

if (!file.exists(dataDir)) {
        dir.create(dataDir)
}

# Now we download the bz2 zip file... and extract it...

if (!file.exists(filePath)) {
        temp <- tempfile()
        download.file(fileUrl, temp, mode="wb", method = "curl")
        bunzip2(temp, filePath, overwrite=TRUE)
        unlink(temp)
}

# Load the dataset in preperation for manipulation...

rawStormData  <- read.csv(filePath)     # Load the raw data...
stormData <- rawStormData               # Create a working copy of the data...

```

# Storm Data Overview

Due to changes in the data collection and processing procedures over time, there are unique periods of record available depending on the event type. The [NOAA Event Database Website][2] details the timeline and different time spans for each period of unique data collection and processing procedures.

- **1950 - 1993** - Prior to 1993, the records are extracted from the manually typed Storm Data Publication. 
- **1993 to the present** - built from the digital records in the database. 
- **Paradox Database Files**: Beginning in January 1996, the NWS began using Storm Data for forecast verification purposes and the NWS decided to collect the data at the NWS Headquarters in Silver Spring, MD. They selected Borland/Corel Paradox format for their database and supplied the NCDC with the raw data files, which were then used for the Storm Data publication and inclusion into the Storm Events Database. **From 1996-1999, the event type field was a free-text field so there were many, many variations of event types. Most of the events were standardized into the 48 current event types**. In 2000 the NWS added a drop-down selector for Event Type on the data entry interface, which standardized the Event Type values sent to NCDC.
- **Comma-Separated Text (CSV) Files**: In October 2006, the NWS switched from Paradox to Windows SQL Server and it was decided that the NWS would supply NCDC with comma separated (CSV) text files that NCDC would import into their own database for the Storm Data publication and Storm Events Database. 

## Data

The principle variables we are interested within the data set are:

1. BGN_DATE: Date event started 
1. EVTYPE: Event Type  
    2.1. **Note:** There are 48 event types defined post 2000
1. FATALITIES: Number of fatalities
1. INJURIES: Number of injuries
1. PROPDMG: Property damage estimates, entered as $ dollar amount.
1. PROPDMGEXP: Alphabetic Codes to signify magnitude “K” for thousands, “M” for millions, and “B” for billions)
1. CROPDMG: Crop damage estimates, entered as $ dollar amounts
1. CROPDMGEXP: Alphabetic Codes to signify magnitude “K” for thousands, “M” for millions, and “B” for billions)

### Event Type 

The [NOAA Event Database Website][2] specifies **48 Event types**, however as discussed the raw data contans the pre-2000 open text field data. After 2000 the data event names are uniform and align with the table below. 

Event Name               | Event Name               | Event Name         | Event Name
-------------------------|--------------------------|--------------------|-----------
Astronomical Low Tide    | Hurricane (Typhoon)      | Avalanche          | Ice Storm
Blizzard                 | Lake-Effect Snow         | Coastal Flood      | Lakeshore Flood
Cold/Wind Chill          | Lightning                | Debris Flow        | Marine Hail  
Dense Fog                | Marine High Wind         | Dust Devil         | Marine Strong Wind 
Drought                  | Marine Thunderstorm Wind | Excessive Heat     | Rip Current  
Dust Storm               | Seiche                   | Flash Flood        | Sleet  
Extreme Cold/Wind Chill  | Storm Surge/Tide         | Frost/Freeze       | Strong Wind 
Flood                    | Thunderstorm Wind        | Freezing Fog       | Tornado  
Funnel Cloud             | Tropical Depression      | Heat               | Tropical Storm
Hail                     | Tsunami                  | Hail               | Volcanic Ash  
Heavy Rain               | Waterspout               | Heavy Snow         | Wildfire  
High Surf                | Winter Storm             | High Wind          | Winter Weather 

# Raw Data - Processing

The actual raw data as downloaded has a number of issues we have to address. 

In terms of actual clean up work the intent is to do this in an orderly manner workign through the fields we have identified, and self checking our approach.

1. BGN_DATE: Date event started 
1. EVTYPE: Event Type  
    2.1. **Note:** There are 48 event types defined post 2000
1. FATALITIES: Number of fatalities
1. INJURIES: Number of injuries
1. PROPDMG: Property damage estimates, entered as $ dollar amount.
1. PROPDMGEXP: Alphabetic Codes to signify magnitude “K” for thousands, “M” for millions, and “B” for billions)
1. CROPDMG: Crop damage estimates, entered as $ dollar amounts
1. CROPDMGEXP: Alphabetic Codes to signify magnitude “K” for thousands, “M” for millions, and “B” for billions)

## BGN_DATE

The first issue we have to address with the raw data is the date format of **BGN_DATE**. The event begin date is specified as a text string. We use a combination of **dplyr** and **lubridate** to create two new columns **eventBeginDate** and **eventBeginYear** (*derived from eventBeginDate*)

Column Name        | Format             | Type
-------------------|--------------------|------
BGN_DATE:          | MM/DD/YYYY H:MM:SS | Character String
eventBeginDate:    | YYYY/MM/DD         | POSIXct Date
eventBeginYear:    | YYYY               | Factor

```{r BGN_DATE_Cleanup, cache=FALSE}

stormData <- mutate(stormData, eventBeginDate = mdy_hms(BGN_DATE,tz=Sys.timezone()))
stormData <- mutate(stormData, eventBeginYear = year(eventBeginDate))

```

## EVTYPE

The next step is to consider `EVTYPE`, the Event Type was standardised in 2000, however prior to this date it was a free text field. A bief check confirms that problems likely exist with the raw data. If we count the number of unique event types we see **`r length(unique(stormData$EVTYPE))`** unique `EVTYPE` values across the raw dataset, where we would hope for **48** values... 

```{r stormDataCount, cache=FALSE}

length(unique(stormData$EVTYPE))

```

### EVTYPE Clean UP

To address the issues with EVTYPE we initially decided to take the the approach of converting all the entries to be a factor that maps to one of the 48 types defined after 2000.  To do this we originally planned to subset the data, before 2000 we would need to clean it up, after 2000 we would want to leave it alone... The basic apprach being to convert EVTYPE data to be in character format, execute a regular expression across the dataset and replace all matches with the EVTYPE replacement value. Once complete we would create a new column, copy, convert and assign the data to the new column. We would then check we have < 48 values. 

It turns out that the post 2000 data also appears to have issues up until 2007. After 2007 we see 48 factors, but we see have **985 factor levels**. On top of this for example we see that in 1996 the EVTYPE was used to add summaries of events using free text... 

The situation requires that we take an alternate approach. We create a set of well defined buckets and aggregate the eventTypes against this. i.e. For every entry we find "hail" in we replace with "HAIL", to do this we use grepl(). Essentially we define a bucketlist (eventFactorName), and matching regular expression list (eventRegex). We use these lists to perform a iteractive search across the evData. Once complete we convert eventType values to be factors. The final step is to extract all the rows that match our factor list and use that as the dataset. 

```{r cache=FALSE}
eventFactorName <- c("AVALANCHE","BLIZZARD","COLD","DUST","FIRE","FREEZING","FLOOD",  
"HAIL","HEAT","HURRICANE","LIGHTNING","RAIN","RAIN","SNOW","STORM SURGE","THUNDERSTORM",
"THUNDERSTORM", "TORNADO","TROPICALSTORM","WIND")

eventRegex <- c("avalanche", "blizzard", "cold", "dust*", "fire", "freez*", "flood",
"hail", "heat", "hurricane", "lightn", "rain", "precip", "snow", "storm surge", "thu*.*orm", 
"tstm", "tornad", "tropical.*", "wind")

eventType <- data.frame(eventFactorName,eventRegex)
eventType

stormData$EVTYPE <- as.character(stormData$EVTYPE) # Convert EVTYPE to a Text Field...
for (i in 1:length(eventFactorName)) 
{
        # Search for eventRegex string - on match replace with eventFactorName    
        stormData$eventType[grepl(eventRegex[i], 
                                  stormData$EVTYPE,
                                  ignore.case = TRUE)] <- eventFactorName[i]
}
stormData$eventType <- as.factor(stormData$eventType) # Convert eventType to be a factor

# Remove all values that are not a defined factor
stormData <- filter(stormData, !is.na(eventType))

```
We can see that we loose some data through this process

Raw stormData:

- `r dim(rawStormData)`

Cleaned stormData 

- `r dim(stormData)`

- **BGN_DATE** is replaced by **eventBeginDate** *and* **eventBeginYear**

- **EVTYPE** is replaced by **eventType**

This process results in a dataset of `r nlevels(stormData$eventType)` event types as detailed in the following table:

```{r Levels, echo=TRUE, results='markup'}

levels(stormData$eventType)

```

### FATALITIES: Cleanup 

Fatalities is a numeric value hence we want to check for and remove any negative values. Here we replace negative values by 0. We also check for any very large values.

```{r cache=FALSE}

stormData$FATALITIES[stormData$FATALITIES<0] <- 0
unique(stormData$FATALITIES)

```
### INJURIES: Cleanup 

Injuries is a numeric value hence we want to check for and remove any negative values. Here we replace negative values by 0. We also check for any very large values.

```{r cache=FALSE}

stormData$INJURIES[stormData$INJURIES<0] <- 0
unique(stormData$INJURIES)

```
### PROPDMG / PROPDMGEXP

Property damage is calculated based off two columns, the first column **PROPDMG** defines the property damage estimates, entered as $ dollar amount. The second column defines an alphabetic exponent code ** PROPDMGEXP*** to signify magnitude “K” for thousands, “M” for millions, and “B” for billions)

Our approach here is to remove any negative values from the PROPDMG column, and to convert the second column to be a numeric column after checking that no bad values exist.

```{r cache=FALSE}
(stormData$PROPDMG[stormData$PROPDMG<0] <- 0)

# Lets count the frequency of the values in stormData$PROPDMGEXP to see what the
# data quality is like.
stormData %>% group_by(PROPDMGEXP) %>% summarise(count=n())

# Our objective is to clean up the data by first replacing all instances of
# "k","m","b" by "K", "M", "B"...
stormData$PROPDMGEXP <- as.character(stormData$PROPDMGEXP)
stormData$PROPDMGEXP[grepl("k", stormData$PROPDMGEXP, ignore.case = TRUE)] <- "K"
stormData$PROPDMGEXP[grepl("m", stormData$PROPDMGEXP, ignore.case = TRUE)] <- "M"
stormData$PROPDMGEXP[grepl("b", stormData$PROPDMGEXP, ignore.case = TRUE)] <- "B"

# OK, we now remove any value that is not "K", "M", "B" and replace with "0"
stormData$PROPDMGEXP[!grepl("[kmb]", stormData$PROPDMGEXP, ignore.case = TRUE)] <- "0"
stormData$PROPDMGEXP <- as.factor(stormData$PROPDMGEXP)

# Result Cleaned Dataset...
stormData %>% group_by(PROPDMGEXP) %>% summarise(count=n())

```

### CROPDMG / CROPDMGEXP 

Crop damage estimates, entered as $ dollar amounts
Alphabetic Codes to signify magnitude “K” for thousands, “M” for millions, and “B” for billions)

```{r cache=FALSE}
stormData$PROPDMG[stormData$PROPDMG<0] <- 0

# Lets count the frequency of the values in stormData$CROPDMGEXP to see what the
# data quality is like.
stormData %>% group_by(CROPDMGEXP) %>% summarise(count=n())

# Our objective is to clean up the data by first replacing all instances of
# "k","m","b" by "K", "M", "B"...
stormData$CROPDMGEXP <- as.character(stormData$CROPDMGEXP)
stormData$CROPDMGEXP[grepl("k", stormData$CROPDMGEXP, ignore.case = TRUE)] <- "K"
stormData$CROPDMGEXP[grepl("m", stormData$CROPDMGEXP, ignore.case = TRUE)] <- "M"
stormData$CROPDMGEXP[grepl("b", stormData$CROPDMGEXP, ignore.case = TRUE)] <- "B"

# OK, we now remove any value that is not "K", "M", "B" and replace with "0"
stormData$CROPDMGEXP[!grepl("[kmb]", stormData$CROPDMGEXP, ignore.case = TRUE)] <- "0"
stormData$CROPDMGEXP <- as.factor(stormData$CROPDMGEXP)

# Result Cleaned Dataset...
stormData %>% group_by(CROPDMGEXP) %>% summarise(count=n())

```

# Data Checks

In this section we present a set of data check we perform on the data prior to caculating the results. These check were previously included inline with the data processing but have been placed here for clarity.



# Results

From this data, we found that, on average across the U.S:

1.
2.
3.

# References




[1]: http://www.noaa.gov/
[2]: http://www.ncdc.noaa.gov/stormevents/details.jsp?type=collection
[3]: https://d396qusza40orc.cloudfront.net/repdata%2Fpeer2_doc%2Fpd01016005curr.pdf
[4]: https://d396qusza40orc.cloudfront.net/repdata%2Fpeer2_doc%2FNCDC%20Storm%20Events-FAQ%20Page.pdf
[5]: https://class.coursera.org/repdata-012 "Reproducible Research"
[6]: https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2 "Storm Data"

