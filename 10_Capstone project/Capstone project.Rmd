---
title: "Data Science Capstone - Yelp Dataset Analysis"
author: "Technophobe1"
date: "November 2, 2015"
output: 
  html_document: 
    highlight: tango
    number_sections: yes
    theme: cerulean
    toc: no
---

```{r setOptions, echo=FALSE, message=FALSE, results='hide', cache=TRUE}
require(knitr)
opts_chunk$set(fig.width = 10, fig.height = 8)
require(knitcitations)
cleanbib()
```

```{r Setup, echo=FALSE, message=FALSE, results='hide'}
setwd("~/Documents/Dropbox/dev/cousera/courses/10_Capstone project")

requiredPackages <- c("devtools","dplyr","tidyr","data.table","ggplot2","ggvis","RMySQL", "jsonlite", "psych", "plyr", "knitr")

ipak <- function(pkg)
{
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg))
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

ipak(requiredPackages)
```

# Predictive Review by Location and Business Type

The problem and question I wish to ask and answer is 

- "In what region of the Yelp Dataset Challenge dataset does the highest density of a particular form of restaurant occur and which region gives the best review to a particular form of restaurant in the context of a particular event such as Mothers Day, or Fathers Day"_. Secondly, _**"Can we predict that restuarants of type X in region Y have a higher probability of a good review?"**_"

The intent is to combine both location, and temporal data and natural language processing to determine if a correlation exists between location, type of restaurant and event. For example can we determine if the hypothesis _**"People in region X, have the highest density of Chinese restuarants, yet give the best reviews to Italian restaurants on Father's Day"**_ is true, false or non-answerable based on the data-set we are using?

## Data Import

```{r Data_Download, echo=FALSE, message=FALSE, cache=TRUE, results='hide'}

dataDir         <- "./data"
fileUrl         <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/yelp_dataset_challenge_academic_dataset.zip"
filePath        <- file.path(dataDir)

# Does the directory Exist? If it does'nt create it
if (!file.exists(dataDir)) {
  dir.create(dataDir)
}

# Now we check if we have downloaded the data already into 
# "./data/yelp_dataset_challenge_academic_dataset". If not, then we download the
# zip file... and extract it under the data directory as 
# './data/yelp_dataset_challenge_academic_dataset'...

if (!file.exists( file.path(dataDir,"yelp_dataset_challenge_academic_dataset"))) {
  temp <- tempfile()
  download.file(fileUrl, temp, mode = "wb", method = "curl")
  unzip(temp, exdir = dataDir)
  unlink(temp)
}
```


```{r Data_Import, echo=FALSE, message=FALSE, cache=TRUE, results='hide'}

# OK, now we have downloaded the data, we now need to import it into R and clean
# it... We have five datasets:
#  
#   1) Business, 2) Checkin, 3) Review, 4) tip, 5) User
# 

# Load the dataset in preperation for manipulation... note that we first check
# to see if the datframe exists as we do not want to reload the data every time
# we compile the markdown file :-)

dataDir <- "./data/yelp_dataset_challenge_academic_dataset"

if ( !exists("yelpBusinessData") )
{
  if (file.exists( file.path(dataDir,"yelpBusinessData.rds"))) {
    yelpBusinessData <- readRDS(file.path(dataDir,"yelpBusinessData.rds"))
  } else {
    yelpBusinessDataFilePath <- file.path(dataDir, 
                                          "yelp_academic_dataset_business.json")
    yelpBusinessData <- fromJSON(sprintf("[%s]",
                                         paste(readLines(yelpBusinessDataFilePath),
                                               collapse = ",")),
                                 flatten = FALSE)
    str(yelpBusinessData, max_level = 1)
    # Fix the column name duplication issue
    # If and when you flatten the data the you create two columns wiht the same column id
    # 
    # i.e. yelpBusinessData$attributes.Good.for.kids
    # 
    # This fixes the issue by renaming the first column...
    colnames(yelpBusinessData$attributes)[7] <- "Good_For_Kids"
    saveRDS( yelpBusinessData, file.path(dataDir, "yelpBusinessData.rds"))
  }
}

if ( !exists("yelpCheckInData") ) {
  if (file.exists( file.path(dataDir,"yelpCheckInData.rds"))) {
    yelpCheckInData <- readRDS( file.path(dataDir, "yelpCheckInData.rds"))
  } else {
    yelpCheckInDataFilePath <- file.path(dataDir, "yelp_academic_dataset_checkin.json")
    yelpCheckInData <- fromJSON(sprintf("[%s]", paste(readLines(yelpCheckInDataFilePath), collapse = ",")), flatten = FALSE)
    str(yelpCheckInData)
    saveRDS( yelpCheckInData, file.path(dataDir, "yelpCheckInData.rds"))
  }
}

if ( !exists("yelpReviewData") ) {
  if (file.exists( file.path(dataDir,"yelpCheckInData.rds"))) {
    yelpReviewData <- readRDS(file.path(dataDir, "yelpReviewData.rds"))
  } else {
    yelpReviewDataFilePath <- file.path(dataDir, "yelp_academic_dataset_review.json")
    yelpReviewData <- fromJSON(sprintf("[%s]", paste(readLines(yelpReviewDataFilePath), collapse = ",")), flatten = FALSE)
    str(yelpReviewData)
    saveRDS( yelpReviewData, file.path(dataDir, "yelpReviewData.rds"))
  }
}

if ( !exists("yelpTipData") ) {
  if (file.exists( file.path(dataDir,"yelpTipData.rds"))) {
    yelpTipData <- readRDS(file.path(dataDir, "yelpTipData.rds"))
  } else {
    yelpTipDataFilePath <- file.path(dataDir, "yelp_academic_dataset_tip.json")
    yelpTipData <- fromJSON(sprintf("[%s]", paste(readLines(yelpTipDataFilePath), collapse = ",")), flatten = FALSE)
    str(yelpTipData)
    saveRDS( yelpTipData, file.path(dataDir, "yelpTipData.rds"))
  }
}

if ( !exists("yelpUserData") ) {
  if (file.exists( file.path(dataDir,"yelpUserData.rds"))) {
     yelpUserData <- readRDS(file.path(dataDir, "yelpUserData.rds"))
  } else {
    yelpUserDataFilePath <- file.path(dataDir, "yelp_academic_dataset_user.json")
    yelpUserData <- fromJSON(sprintf("[%s]", paste(readLines(yelpUserDataFilePath), collapse = ",")), flatten = FALSE)
    str(yelpUserData)
    saveRDS( yelpUserData, file.path(dataDir, "yelpUserData.rds"))
  }
}
```

```{r create_database, echo=FALSE, message=FALSE, cache=TRUE, results='hide'}
# We now have the data loaded... Next step is to move the data into a MySql 
# database. This is good practice as we can then use dplyr to access and
# manipulate the data without having to keep all the tables in memory...

# The dbConnect command below connects the database, rather than putting the
# password and data in the file, like so:
# 
# conDB <- dbConnect(RMySQL::MySQL(),
#                 user = 'root',
#                 password = 'SecurityRisk!!!!',
#                 host = '127.0.0.1',
#                 port = 3306,
#                 dbname = 'Capstone')
#
# we create a seperate '~/.my.cnf' file, which we place in the users top level
# directory, we then access the data via the database name...
# 
# Example '~/.my.cnf' file...
# 
# [client]
# user = root
# host = localhost
# password = NotTelling!!!!!
# [theDatabase]
# database = Capstone
#
# Now we just access the database by name and no password is exposed in the
# source code...
# 
conDB <- dbConnect(RMySQL::MySQL(), group = 'CapstoneDB')

# The Next step is to flatten each file and then write it to the MySQL database 
# 'Capstone' we created earlier. Note that we use Mamp Pro standard settings. i.e. 
# Launch MAMP Pro, launch RSTUDIO...
# 
# OK, now write the tables out to the database 'Capstone'
# 
# We check to see if the tables exist in Capstone as we do not want to re-create
# if they already exists...
# 
if (!dbExistsTable(conDB, "yelpBusinessData") ) {
  yelpBusinessDataFlat <- flatten(yelpBusinessData, recursive = TRUE)
  yelpBusinessDataFlat$categories <- sapply(yelpBusinessDataFlat$categories, toString)
  yelpBusinessDataFlat$neighborhoods <- sapply(yelpBusinessDataFlat$neighborhoods, toString)
  yelpBusinessDataFlat$`attributes.Accepts Credit Cards` <- sapply(yelpBusinessDataFlat$`attributes.Accepts Credit Cards`, toString)
  dbWriteTable(conn = conDB, name = 'yelpBusinessData', value = as.data.frame(yelpBusinessDataFlat), overwrite = TRUE)
  rm(yelpBusinessDataFlat) # Save Space
}


if (!dbExistsTable(conDB, "yelpCheckInData") ) {
  yelpCheckInDataFlat <- flatten(yelpCheckInData, recursive = TRUE)
  dbWriteTable(conn = conDB, name = 'yelpCheckInData', value = as.data.frame(yelpCheckInDataFlat), overwrite = TRUE)
  rm(yelpCheckInDataFlat) # Save Space
}

if (!dbExistsTable(conDB, "yelpReviewData") ) {
  yelpReviewDataFlat <- flatten(yelpReviewData, recursive = TRUE)
  dbWriteTable(conn = conDB, name = 'yelpReviewData', value = as.data.frame(yelpReviewDataFlat), overwrite = TRUE)
  rm(yelpReviewDataFlat) # Save Space
}

if (!dbExistsTable(conDB, "yelpTipData") ) {
  yelpTipDataFlat <- flatten(yelpTipData, recursive = TRUE)
  dbWriteTable(conn = conDB, name = 'yelpTipData', value = as.data.frame(yelpTipDataFlat), overwrite = TRUE)
  rm(yelpTipDataFlat) # Save Space
}

if (!dbExistsTable(conDB, "yelpUserData") ) {
  yelpUserDataFlat <- flatten(yelpUserData, recursive = TRUE)
  yelpUserDataFlat$friends <- sapply(yelpUserData$friends, toString)
  yelpUserDataFlat$elite <- sapply(yelpUserData$elite, toString)
  rm(yelpTipDataFlat) # Save Space
  
  dbWriteTable(conn = conDB, name = 'yelpUserData', value = as.data.frame(yelpUserDataFlat), overwrite = TRUE)
}

dbListTables(conDB)
dbDisconnect(conDB)

```
## Exploratory Analysis


The yelp dataset includes:

- 1.6M reviews and 500K tips by 366K users for 61K businesses
- 481K business attributes, e.g., hours, parking availability, ambience.
- Social network of 366K users for a total of 2.9M social edges.
- Aggregated check-ins over time for each of the 61K businesses

The datset covers 10 cities...
 
- U.K.: Edinburgh
- Germany: Karlsruhe
- Canada: Montreal and Waterloo
- U.S.: Pittsburgh, Charlotte, Urbana-Champaign, Phoenix, Las Vegas, Madison

Our goal here is to do an initial mapping of the geo data, nothing to complex our goals are to explore the data. We start with a macro view, what cities do we have in the data set? From that we can then start to look at the geographic data...
 
```{r InitialAnalysis, echo=FALSE, message=FALSE, cache = TRUE, results = 'hide'}

require("maps")
require("ggmap")
require("psych")
require("maptools")
set.seed(777)

# The Yelp Dataset describes 10 Cities, is that true?
# Lets partition the yelp data set into 10 clusters and then map those cluster to location

geo.cluster <- kmeans(yelpBusinessData[,c('longitude','latitude')], 10)
geo.cluster$centers

# We can see that the goe.clusters result skews to the US data due to relative size of cluster...
```

```{r Mapping, echo = FALSE, message = FALSE, cache = TRUE, fig.width=12, fig.height=8, results = 'hide'}

cities <- c('Edinburgh, UK', 
          'Karlsruhe, Germany', 
          'Montreal, Canada', 'Waterloo, Canada', 
          'Pittsburgh, PA', 'Charlotte, NC', 'Urbana-Champaign, IL', 'Phoenix, AZ', 'Las Vegas, NV', 'Madison, WI')

city.centres <- geocode(cities, source = "google")
geo.cluster <- kmeans(yelpBusinessData[,c('longitude','latitude')],city.centres)
# summary(geo.cluster)

# Now we can map these clusters to a map of the world to get a sense of the 
# data... Here what we do os map the cluster centers to the map and scale the 
# point size to the cluster size of the kmeans clusters...
# 
# Note the relative size of the clusters - Phoenix data set is considerably
# larger than the UK and German data sets...
# 
# 
gp <- ggplot(data.frame(geo.cluster$centers), 
             aes(longitude, latitude, color = cities))
gp <- gp + borders("world", background = "lightblue", fill = "white", col = "cornflowerblue") 
gp <- gp + geom_point(aes(size = geo.cluster$size))
gp <- gp + geom_text(aes(city.centres$lon, city.centres$lat, label = cities), 
                     size = 3, hjust = -0.2, vjust = .3, color = "black")
gp
rm(gp)

require(dplyr)
# convert to numeric
clusterAnalysis <- data.frame(cbind(City = cities, Cluster = geo.cluster$size))
clusterAnalysis$Cluster <- as.numeric(as.character(clusterAnalysis$Cluster))

```

What we find is that the size of the relative geospatial datasets sku's the initial result. There is not an even distribution of data by location. The map displayed in the figure above depicts and addresses this problem, the table below shows the cluster sizes. We obtain the data by pulling the geocodes for the cities referenced and use these codes to create the cluster buckets. Note that if we sum the buckets we get: `r format(sum(clusterAnalysis$Cluster), scientific=FALSE)` , which aligns with the sampe row size of `yelpBusinessData`

```{r, echo = FALSE, results='asis'}
require(knitr)
kable(arrange(clusterAnalysis,Cluster))
```

Now we have a sense of the data clusters and their sizes, lets take a look at the next component of the proposed analysis.

- In what region of the Yelp Challenge dataset do the highest priced restuarants occcur?
- In what region of the Yelp Challenge dataset does the highest density of a particular form of restaurant occur?
- Which region gives the best review to a particular form of restaurant?

for the first question the intent is to investigate the types of restuarant by cluster, and then prioratise them by cluster. We can then look at the type of restuarant as a % of restuarants by cluster to determine popularity aginst rating.

For example, we can create a subset for Phoenix and create a cluster map that examines the price denisity of restuarants. Lookign at the data we see that there is a cluster of expensive restaurants downtown. Looking at the data we see:

```{r, echo = FALSE, message = FALSE, error = FALSE}
require(knitr)
require(ggmap)
require(dplyr)
require(pander)

map <- get_map(location = 'Phoenix, AZ', zoom = 10, maptype = "roadmap")
yelpBusinessData$city <- as.factor(yelpBusinessData$city)
yelpBusinessData.PA <- yelpBusinessData[which(yelpBusinessData$city == "Phoenix"),]
# Only restaurants (as in category)
yelpBusinessData.PA.rest <- yelpBusinessData.PA[which(grepl("Restaurants", yelpBusinessData.PA$categories)),]

kable( describe(yelpBusinessData.PA.rest$attributes$`Price Range`), digits = 2 )
pander(count( yelpBusinessData.PA.rest$attributes$`Price Range` ))

```

```{r PheonixPriceDensity, echo = FALSE, message = FALSE, error = FALSE, warning = FALSE}

mP <- ggmap(map, extent = "device", legend = "topright")
mP <- mP + geom_point( data = yelpBusinessData.PA.rest,
                      aes( x = yelpBusinessData.PA.rest$longitude,
                           y = yelpBusinessData.PA.rest$latitude,
                           color = yelpBusinessData.PA.rest$attributes$`Price Range`),
                      size =  yelpBusinessData.PA.rest$attributes$`Price Range`,
                      alpha = 0.5 )

# Set the Legend to be a continous colour bar... 
mP <- mP + scale_colour_continuous(low = "blue", 
                                   high = "red", 
                                   space = "Lab", 
                                   guide = "colorbar", 
                                   name = "Price Range")
mP

rm(mP)

```

OK, where are those four very expensive restaurants?...




## Methods and Data
- Describe how you used the data and the type of analytic methods that you used; it's okay to be a bit technical here but clarity is important

## Results 
- Describe what you found through your analysis of the data.

## Discussion
- Explain how you interpret the results of your analysis and what the implications are for your question/problem.

